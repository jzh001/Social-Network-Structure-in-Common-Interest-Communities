{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize,sent_tokenize,Text,PorterStemmer,WordNetLemmatizer,pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "import collections as clt\n",
    "import time\n",
    "from community import best_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews_full_v5.csv')\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7) #defining size of plots\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    i originally bought chaosphere in late 2000 an...\n",
      "1    a midpoint between carcass early grind albums ...\n",
      "2    im not hardcore into metal at all really but i...\n",
      "3    this is not a new album but in fact five album...\n",
      "4    ive been trying for months to come up with an ...\n",
      "5    andromeda extension of the wish clearly one of...\n",
      "6    to start off i must say that i did not always ...\n",
      "7    psycroptic  the isle of disenchantment self re...\n",
      "8    absolute steel  the fair bitch project edgerun...\n",
      "9    purgation  realm of the dead mcdself released ...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['Text'] = df['Text'].str.lower().str.replace(\"'s\",'').str.replace(\"'\",'').str.replace('-','').str.replace('(',')').str.replace(')','').str.replace(',','').str.replace('.','')\n",
    "print(df['Text'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9569 88382 91.55017757415771\n"
     ]
    }
   ],
   "source": [
    "B = nx.Graph()\n",
    "#stemmer = PorterStemmer() #faster\n",
    "#stemmer = WordNetLemmatizer() #slower but more accurate\n",
    "vocab = set()\n",
    "a = time.time()\n",
    "spell = SpellChecker()\n",
    "N_albums = len(set(df.Release))\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "for index, row in df.head(5000).iterrows():\n",
    "    text = pos_tag(word_tokenize(row['Text']))\n",
    "    album = row['Release'] + 'A'\n",
    "    #print(text)\n",
    "    for word,tag in text:\n",
    "        #print(word)\n",
    "        word = word.lower()\n",
    "        if word.isalpha() and len(word) > 4 and len(word) < 15 and tag == 'JJ' and word not in all_stopwords and len(spell.unknown([word])) == 0:\n",
    "            if not B.has_edge(album, word):\n",
    "                B.add_edge(album, word, weight = 0)\n",
    "            B[album][word]['weight'] += 1\n",
    "            vocab.add(word)\n",
    "print(B.number_of_nodes(), B.number_of_edges(),time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for word in vocab:\n",
    "    for album in B.adj[word]:\n",
    "        for word1 in B.adj[album]:\n",
    "            if word < word1: #order is important to avoid repeats\n",
    "                if not G.has_edge(word,word1):\n",
    "                    G.add_edge(word, word1, weight = 0)\n",
    "                G[word][word1]['weight'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df = pd.read_csv('reviewsv7.csv')\n",
    "mp = {}\n",
    "for index, row in df.iterrows():\n",
    "    mp[(row['User'], row['Release'])] = (row['Text'],row['Sentiment'])\n",
    "\n",
    "df = pd.read_csv('reviews_full_v3.csv')\n",
    "text = []\n",
    "senti = []\n",
    "for index, row in df.iterrows():\n",
    "    if (row['User'], row['Release']) in mp:\n",
    "        text.append(mp[(row['User'], row['Release'])][0])\n",
    "        senti.append(mp[(row['User'], row['Release'])][1])\n",
    "    else:\n",
    "        text.append(np.nan)\n",
    "        senti.append(np.nan)\n",
    "df['Text'] = text\n",
    "df['Sentiment'] = senti\n",
    "df.to_csv('reviews_full_v4.csv', index = False)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
